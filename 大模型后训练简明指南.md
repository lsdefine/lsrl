# 大模型后训练简明指南

## 基础：Forward-Backward-Step

无论你是在进行监督微调（SFT）、指令微调（IFT），还是强化学习（RL）的策略训练，所有的模型训练本质上都遵循同一个基本循环：​**Forward-Backward-Step**​。只有通过这三步才能更新模型参数。

### 三步走的本质

#### Forward（前向传播）

* **输入数据**通过模型层层计算，产生输出
* 计算​**损失函数**​（如交叉熵损失、策略损失等）
* 这一步主要消耗显存存储**激活值**

#### Backward（反向传播）

* 从损失开始，逐层计算**梯度**
* 梯度会累积到各个参数上
* 这一步主要消耗显存存储**梯度**

#### Step（参数更新）

* 使用优化器（如Adam）**更新模型参数**
* 可能涉及梯度裁剪、学习率调度等
* 这一步主要消耗显存存储**优化器状态**

```python
for batch in dataloader:
	loss = model(batch, labels=batch).loss
	loss.backward()
	opt.step()
```

## 资源规划：显存占用与时间估算

问题：SFT训练一个7B/14B模型到底需要多少张卡？

### 每一步骤的显存占用

首先，目前成熟的模型训练均以BF16为基本格式。

BF16下，一个7B模型的模型参数占用**14GB**存储。

每一步的显存占用：

- Forward：模型参数（**14GB**） +  前向激活值（**长度相关**）
- Backward：模型参数 （**14GB**）+ 前向激活值（**长度相关**）+ 参数梯度（**14GB**）
- Step：模型参数 （**14GB**）+ 参数梯度（**14GB**）+ 激活器动量（**56GB！**）
- 第二次Forward（梯度累积下）：模型参数（**14GB**） + 参数梯度（**14GB**）+ 前向激活值（**长度相关**）

最大需要占用84GB显存，超过单卡80G。

对于一个14B模型，其参数占用**28GB**。

- Forward：模型参数（**28GB**） +  前向激活值（**长度相关**）
- Backward：模型参数 （**28GB**）+ 前向激活值（**长度相关**）+ 参数梯度（**28GB**）
- Step：模型参数 （**28GB**）+ 参数梯度（**28GB**）+ 激活器动量（**112GB！**）
- 第二次Forward（梯度累积下）：模型参数（**28GB**） + 参数梯度（**28GB**）+ 前向激活值（**长度相关**）

> 可以开个nvitop对照着看，可以看到差不多的显存占用情况。
> 这里的长度相关实际上指的是Batch大小相关。1* 8k ~= 2 * 4k， etc。（长度在几k的时候attention的二次方其实不占主导地位）

在这一计算下，可以看到，即使是7B模型在step的时候也完全无法放进单张80G显卡中。

而模型训练能支持的长度，则由参数和梯度之外余下的显存空间决定。

### 优化器状态（激活器动量）

Adam优化器需要存储每个参数的momentum和variance，为参数量的2倍。

最为致命的是，这两个优化器状态需要以FP32格式存储！所以存储空间占用再次翻倍。

> 所谓的8bit Adam之类的方法会将优化器状态以更低精度进行存储，可以避免翻倍。但是显然据计算还是放不下。而且会产生精度损失。

### 梯度累积

一般来说，一个成熟的大模型训练一次参数更新所使用的梯度，要来自于100万个（甚至1000万个）以上的tokens。

这个值实际上在大模型训练中几乎是最重要的参数，但是它并没有一个公认的名字，一些人叫它real_batch_size以和传统的batch_size区分。

可以很容易的从定义上看到，real_batch_size = length * batch_size （* 卡数）。

但是就算不考虑其他占用，也不可能存放下那么多的tokens关联的前向激活值。

因此需要有梯度累积，意思就是forward-backward得到一个Batch对应的梯度以后，实际上step什么也不做，只是把这个梯度累计起来。直到累计了accum_steps个Batch产生的梯度以后，才将综合计算的梯度来更新模型参数。

因此，real_batch_size = length * batch_size * DP卡数 * accum_steps

在这种情况下，step()实际上每调用accum_steps次才会更新一次模型参数。

> lsrl包和很多教程中的默认accum_steps都过小，实际上可以代入上面公式算算一般需要多大的accum_steps才能把real_batch_size顶到100万以上。
> 
> 某些框架很鸡贼地装作它们可以支持上千的batch_size，实际上它们内部也是拆分以后进行accumulate，一次forward不可能塞入上千的batch_size对应的模型激活值。而且巨大的real_batch_size几乎总会得到更好的结果。
> 
> 真实使用请用大accum_steps！
> 
> 真实使用请用大accum_steps！

### gradient_checkpoint

开启gradient_checkpoint以后，可以大幅度降低前向激活值的存储占用。

其原理是将本该存储的激活值不存储，而是在backward的时候重新计算。

显然可以看出，它几乎需要**翻倍**的Forward时间，但是可以大幅减少**前向激活值部分**的存储占用。

在上述存储占用分析中，其实可以发现空间上的压力比时间要大。

所以，基本上gradient_checkpoint必须开启。

但是，即使开启了gradient_checkpoint，14B模型在backward的时候，80GB显存也只剩下24G显存放前向激活值部分，也放不了多长。

> 那么难道一定要很多张卡进行训练吗？

### 时间空间平衡

可以看到优化器状态的存储占用甚至比模型本身还要大得多。

模型的计算（forward和backward）必须使用GPU进行计算。但是优化器呢？

> 优化器状态相关的计算几乎都是逐元素的加法、乘法和赋值。实际上对计算能力的需求没那么高！
> 
> 在这一设定下，把优化器状态从GPU显存中剔除几乎是最合理的选择。
> 
> 顺带一提，如果你在使用LLaMA-Factory或者DeepSpeed等其他框架的时候没有注意保存模型时的设定而也保存了优化器状态，那么即使是硬盘空间都会飞快地被巨大的优化器状态消耗掉。

### 优化器Offload

解决方案：将模型复制一个副本放在内存中，将Adam优化器绑定在这个模型的内存副本上，这样优化器状态也会自然地产生在内存中。

在这种情况下，Forward和Backward仍然在GPU中进行。但是在Step之前，我们需要先从GPU中拷贝（其实是剪切）梯度到内存的副本中，然后优化器的更新计算完全在CPU中进行。

相比标准的设计，我们产生了如下时间成本：

- 将梯度拷贝到内存的时间
- 使用CPU而非GPU进行模型更新计算的时间
- 将模型参数拷贝回GPU的时间

这些成本用于交换**112GB**的显存占用（14B模型下），实际上完全划算！

- 在accumulate的设定下，真正的参数更新（以及这些附加的传输）实际上每很多步Forward-Backward才会真正执行一次。
- 优化器的计算大多都是简单的逐元素计算，实际上CPU也不慢。（注意lsrl的提示，在多卡环境下需要设置好OMP环境变量）。
- CPU-GPU传输会使用pinned-memory，实际上非常快！（28GB传输，实测GPU-CPU需要2秒，CPU-GPU只需0.几秒）
- lsrl实现中没有在初始化的时候pinned-memory，这样初始化时间大大减少，但是前两次拷贝速度会比较慢，但是第三次拷贝的时候torch会识别到自动pinned上memory，之后数据传输会非常快。

用法非常简单：只需要用lsrl的CPUAdamW替代原生的torch.optim.AdamW即可。

### grad offload

解决了step部分的优化器显存占用问题，还会看到一个奇怪的现象。

> 模型第一次forward和backward正常。
> 
> 因为有accumulate，所以第一次step实际上什么也没做（只累加了梯度）。
> 
> 模型第二次forward相同长度的数据，OOM。

从上面的显存占用分析，可以非常容易地找到这种奇怪现象的原因。

> 第一次forward的时候，param.grad is None。
> 
> 第二次forward的时候，因为需要累加梯度，param.grad是一个tensor，占用了一倍的显存空间。实际上用于前向激活值的显存空间大大减少了。

反正在内存上已经有一个模型参数和梯度的拷贝了，那么，可以在每次backward以后，都**立刻**把梯度剪切到内存上并进行累加，拷贝完后把GPU上的模型梯度设置为None。这样每次forward都像第一次forward一样。

这样每次forward-backward都需要进行一次28GB的拷贝（14B模型下），但是如前所述，在pinned-memory中这个拷贝很快。

而换来的是，整整一倍模型占用空间的显存都可以用来放前向激活值，在gradient_checkpoint的帮助下，支持长度能长好几k！

> 为什么我们对forward-backward一次支持的长度这么在意？
> 
> 因为**上下文的完整性**对模型训练至关重要。虽然梯度累积可以让我们用小batch训练，但如果单次forward的序列长度太短，会带来两个问题。
> 
> 1. **破坏语义连贯性**：模型无法在一个完整的上下文中学习，被迫从截断的片段中学习，这会损害模型对长文本的理解能力。同时，训练时的最大长度也定义了模型在推理时的能力上限。
> 2. **降低训练效率**：更短的序列意味着需要更多的累积步骤才能达到相同的real\_batch\_size，这会增加通信开销和训练时间。

## 多卡：DP，PP与TP

> 一个厨师煎一个蛋需要1分钟，两个厨师1分钟能煎几个蛋？
>
> 一个厨师负责煎正面需要半分钟，另一个厨师负责煎反面也需要半分钟，那么两个厨师一起上能比一个厨师的速度翻倍吗？

在分布式计算中，有一个很重要的原理：N个worker相比于1个worker的理论加速比上限是N倍，但是基本达不到，因为还有通信和同步损失。

极端情况下（不合理的分配下），N个worker甚至可能比1个worker更慢。（三个和尚没水吃！）

在使用多卡时，请确保你清楚地知道这些卡都在干什么！

> 当看到有人把7B模型切分到4张80G卡上做短序列推理还抱怨为什么这么慢，我应该给他批8卡吗？

### DP：Data Parallelism

**数据并行**指的是，每一张卡都有能力单独处理一批数据，这样8张卡就可以在相同时间内处理8批不同的数据。

每张卡都需要存放一个模型的副本，当模型参数更新时，每张卡都需要同步更新以保证任意时刻每张卡上的模型内容都一样。

因此，通信只在模型更新时产生，需要分发新的模型参数。按照前面的accum_steps的讨论，如果实际上accum_steps足够大（如256），那么数据通信的频率非常低。

还有等待成本，如果一些卡跑的更快，那么在模型更新的时候需要等待最慢的卡完成它的任务。但是在这么低的同步频率下，这个成本的占比也非常小。

所以，DP几乎能达到N倍的加速比上限！

> 如果开了grad offload，那么每次forward-backward都需要收集一次梯度，虽然通信成本增加了，但是仍然能一次处理8批数据，仍然具有相当高的加速比（7.xx倍）。
>
> 但是如果训练数据变长很严重的话，等待成本可能会增加。

然而，DP的前提是**每一张卡都有能力单独处理一批数据**。所以本教程前面对于单卡情况进行了大量讨论。

DP是几乎能达到N倍加速，那另外两种是不是更强呢？很遗憾，它们甚至连1倍都达不到，也就是比单卡更慢。

这也是为什么前面我们讨论了这么多时间与空间的权衡（事实上这是计算机科学的永恒话题）：
> 时间慢点就慢点，空间超过了就没法跑。
>
> 空间能用满必须用满，时间最好没有一点浪费。

> 小提示：torch要启动多卡不能使用python xx.py来启动，而要用torchrun --nproc-per-node=N xx.py来启动。或者其他框架会对这个包装一下，如deepspeed xx.py。

### PP：Pipeline Parallelism

**流水线并行**指的是，在一张卡没有能力供一个模型处理数据的时候，将模型按照层切分到多张卡上（深度模型就是多层模型），来共同处理一批数据。

很显然，首先它一次只能处理一批数据，其次在处理这批数据的时候，每当下一层跨卡就需要进行通信。因此，其时间加速比比1x还要低。

一个好消息是现代GPU通信速度非常快，但是再快也就是加速比（实际是减速）接近于1，和DP还是天壤之别。

因此，PP实际上是在**一张卡没有能力供一个模型**时的无奈之举，要是单卡能做到肯定选择DP。

在模型更新的时候，因为每张卡上有一个模型的部分，它们各自更新自己部分就ok，但是正如前面分析的，大accum_steps情况下，模型更新时的通信忽略不计也无伤大雅。

PP是否越多卡越好？显然不是，模型切的越碎，通信成本越高。省越多的卡出来，可以跑dp+pp。

> 时间慢点就慢点，空间超过了就没法跑。
>
> lsrl中的patch_for_multi_gpus.py给出了一个pp的实现。

### TP：Tensor Parallelism



## RL

施工中

## 👏 Citation

If you find this useful, please consider citing our work as follows:

```
@misc{LSRL,
  author = {Jiaqing Liang},
  title = {LSRL: Memory Efficient Large Model Training Framework},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/lsdefine/lsrl}},
}
```
