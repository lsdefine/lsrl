# 大模型后训练简明指南

## 基础：Forward-Backward-Step

无论你是在进行监督微调（SFT）、指令微调（IFT），还是强化学习（RL）的策略训练，所有的模型训练本质上都遵循同一个基本循环：​**Forward-Backward-Step**​。只有通过这三步才能更新模型参数。

### 三步走的本质

#### Forward（前向传播）

* **输入数据**通过模型层层计算，产生输出
* 计算​**损失函数**​（如交叉熵损失、策略损失等）
* 这一步主要消耗显存存储**激活值**

#### Backward（反向传播）

* 从损失开始，逐层计算**梯度**
* 梯度会累积到各个参数上
* 这一步主要消耗显存存储**梯度**

#### Step（参数更新）

* 使用优化器（如Adam）**更新模型参数**
* 可能涉及梯度裁剪、学习率调度等
* 这一步主要消耗显存存储**优化器状态**

```python
for batch in dataloader:
	loss = model(batch, labels=batch).loss
	loss.backward()
	opt.step()
```

## 资源规划：显存占用与时间估算

问题：SFT训练一个7B/14B模型到底需要多少张卡？

### 每一步骤的显存占用

首先，目前成熟的模型训练均以BF16为基本格式。

BF16下，一个7B模型的模型参数占用**14GB**存储。

每一步的显存占用：

- Forward：模型参数（**14GB**） +  前向激活值（**长度相关**）
- Backward：模型参数 （**14GB**）+ 前向激活值（**长度相关**）+ 参数梯度（**14GB**）
- Step：模型参数 （**14GB**）+ 参数梯度（**14GB**）+ 激活器动量（**56GB！**）
- 第二次Forward（梯度累积下）：模型参数（**14GB**） + 参数梯度（**14GB**）+ 前向激活值（**长度相关**）

最大需要占用84GB显存，超过单卡80G。

对于一个14B模型，其参数占用**28GB**。

- Forward：模型参数（**28GB**） +  前向激活值（**长度相关**）
- Backward：模型参数 （**28GB**）+ 前向激活值（**长度相关**）+ 参数梯度（**28GB**）
- Step：模型参数 （**28GB**）+ 参数梯度（**28GB**）+ 激活器动量（**112GB！**）
- 第二次Forward（梯度累积下）：模型参数（**28GB**） + 参数梯度（**28GB**）+ 前向激活值（**长度相关**）

> 可以开个nvitop对照着看，可以看到差不多的显存占用情况。
> 这里的长度相关实际上指的是Batch大小相关。1* 8k ~= 2 * 4k， etc。（长度在几k的时候attention的二次方其实不占主导地位）

在这一计算下，可以看到，即使是7B模型在step的时候也完全无法放进单张80G显卡中。

而模型训练能支持的长度，则由参数和梯度之外余下的显存空间决定。

### 优化器状态（激活器动量）

Adam优化器需要存储每个参数的momentum和variance，为参数量的2倍。

最为致命的是，这两个优化器状态需要以FP32格式存储！所以存储空间占用再次翻倍。

> 所谓的8bit Adam之类的方法会将优化器状态以更低精度进行存储，可以避免翻倍。但是显然据计算还是放不下。而且会产生精度损失。

### 梯度累积

一般来说，一个成熟的大模型训练一次参数更新所使用的梯度，要来自于100万个（甚至1000万个）以上的tokens。

这个值实际上在大模型训练中几乎是最重要的参数，但是它并没有一个公认的名字，一些人叫它real_batch_size以和传统的batch_size区分。

可以很容易的从定义上看到，real_batch_size = length * batch_size （* 卡数）。

但是就算不考虑其他占用，也不可能存放下那么多的tokens关联的前向激活值。

因此需要有梯度累积，意思就是forward-backward得到一个Batch对应的梯度以后，实际上step什么也不做，只是把这个梯度累计起来。直到累计了accum_steps个Batch产生的梯度以后，才将综合计算的梯度来更新模型参数。

因此，real_batch_size = length * batch_size * DP卡数 * accum_steps

在这种情况下，step()实际上每调用accum_steps次才会更新一次模型参数。

> lsrl包和很多教程中的默认accum_steps都过小，实际上可以代入上面公式算算一般需要多大的accum_steps才能把real_batch_size顶到100万以上。
> 
> 某些框架很鸡贼地装作它们可以支持上千的batch_size，实际上它们内部也是拆分以后进行accumulate，一次forward不可能塞入上千的batch_size对应的模型激活值。而且巨大的real_batch_size几乎总会得到更好的结果。
> 
> 真实使用请用大accum_steps！
> 
> 真实使用请用大accum_steps！

### gradient_checkpoint

开启gradient_checkpoint以后，可以大幅度降低前向激活值的存储占用。

其原理是将本该存储的激活值不存储，而是在backward的时候重新计算。

显然可以看出，它几乎需要**翻倍**的Forward时间，但是可以大幅减少**前向激活值部分**的存储占用。

在上述存储占用分析中，其实可以发现空间上的压力比时间要大。

所以，基本上gradient_checkpoint必须开启。

但是，即使开启了gradient_checkpoint，14B模型在backward的时候，80GB显存也只剩下24G显存放前向激活值部分，也放不了多长。

> 那么难道一定要很多张卡进行训练吗？

### 时间空间平衡

可以看到优化器状态的存储占用甚至比模型本身还要大得多。

模型的计算（forward和backward）必须使用GPU进行计算。但是优化器呢？

> 优化器状态相关的计算几乎都是逐元素的加法、乘法和赋值。实际上对计算能力的需求没那么高！
> 
> 在这一设定下，把优化器状态从GPU显存中剔除几乎是最合理的选择。
> 
> 顺带一提，如果你在使用LLaMA-Factory或者DeepSpeed等其他框架的时候没有注意保存模型时的设定而也保存了优化器状态，那么即使是硬盘空间都会飞快地被巨大的优化器状态消耗掉。

### 优化器Offload

解决方案：将模型复制一个副本放在内存中，将Adam优化器绑定在这个模型的内存副本上，这样优化器状态也会自然地产生在内存中。

在这种情况下，Forward和Backward仍然在GPU中进行。但是在Step之前，我们需要先从GPU中拷贝（其实是剪切）梯度到内存的副本中，然后优化器的更新计算完全在CPU中进行。

相比标准的设计，我们产生了如下时间成本：

- 将梯度拷贝到内存的时间
- 使用CPU而非GPU进行模型更新计算的时间
- 将模型参数拷贝回GPU的时间

这些成本用于交换**112GB**的显存占用（14B模型下），实际上完全划算！

- 在accumulate的设定下，真正的参数更新（以及这些附加的传输）实际上每很多步Forward-Backward才会真正执行一次。
- 优化器的计算大多都是简单的逐元素计算，实际上CPU也不慢。（注意lsrl的提示，在多卡环境下需要设置好OMP环境变量）。
- CPU-GPU传输会使用pinned-memory，实际上非常快！（28GB传输，实测GPU-CPU需要2秒，CPU-GPU只需0.几秒）
- lsrl实现中没有在初始化的时候pinned-memory，这样初始化时间大大减少，但是前两次拷贝速度会比较慢，但是第三次拷贝的时候torch会识别到自动pinned上memory，之后数据传输会非常快。

用法非常简单：只需要用lsrl的CPUAdamW替代原生的torch.optim.AdamW即可。

### grad offload

解决了step部分的优化器显存占用问题，还会看到一个奇怪的现象。

> 模型第一次forward和backward正常。
> 
> 因为有accumulate，所以第一次step实际上什么也没做（只累加了梯度）。
> 
> 模型第二次forward相同长度的数据，OOM。

从上面的显存占用分析，可以非常容易地找到这种奇怪现象的原因。

> 第一次forward的时候，param.grad is None。
> 
> 第二次forward的时候，因为需要累加梯度，param.grad是一个tensor，占用了一倍的显存空间。实际上用于前向激活值的显存空间大大减少了。

反正在内存上已经有一个模型参数和梯度的拷贝了，那么，可以在每次backward以后，都**立刻**把梯度剪切到内存上并进行累加，拷贝完后把GPU上的模型梯度设置为None。这样每次forward都像第一次forward一样。

这样每次forward-backward都需要进行一次28GB的拷贝（14B模型下），但是如前所述，在pinned-memory中这个拷贝很快。

而换来的是，整整一倍模型占用空间的显存都可以用来放前向激活值，在gradient_checkpoint的帮助下，支持长度能长好几k！

> 为什么我们对forward-backward一次支持的长度这么在意？
> 
> 因为**上下文的完整性**对模型训练至关重要。虽然梯度累积可以让我们用小batch训练，但如果单次forward的序列长度太短，会带来两个问题。
> 
> 1. **破坏语义连贯性**：模型无法在一个完整的上下文中学习，被迫从截断的片段中学习，这会损害模型对长文本的理解能力。同时，训练时的最大长度也定义了模型在推理时的能力上限。
> 2. **降低训练效率**：更短的序列意味着需要更多的累积步骤才能达到相同的real\_batch\_size，这会增加通信开销和训练时间。

### 一个额外的分析：LoRA

对于一个14B模型，LoRA情况下的显存占用分析：

模型参数（**28GB**） +  前向激活值（**长度相关**）+ 参数梯度（**仅LoRA部分**）+ 激活器动量（**仅LoRA部分**）

问题是，参数梯度和激活器动量我都offload掉了。所以在lsrl的框架下没有必要使用LoRA。

## 多卡：DP，PP与TP

> 一个厨师煎一个蛋需要1分钟，两个厨师1分钟能煎几个蛋？
>
> 一个厨师负责煎正面需要半分钟，另一个厨师负责煎反面也需要半分钟，那么两个厨师一起上能比一个厨师的速度翻倍吗？

在分布式计算中，有一个很重要的原理：N个worker相比于1个worker的理论加速比上限是N倍，但是基本达不到，因为还有通信和同步损失。

极端情况下（不合理的分配下），N个worker甚至可能比1个worker更慢。（三个和尚没水吃！）

在使用多卡时，请确保你清楚地知道这些卡都在干什么！

> 当看到有人把7B模型切分到4张80G卡上做短序列推理还抱怨为什么这么慢，我应该给他批8卡吗？

### DP：Data Parallelism

**数据并行**指的是，每一张卡都有能力单独处理一批数据，这样8张卡就可以在相同时间内处理8批不同的数据。

每张卡都需要存放一个模型的副本，当模型参数更新时，每张卡都需要同步更新以保证任意时刻每张卡上的模型内容都一样。

因此，通信只在模型更新时产生，需要分发新的模型参数。按照前面的accum_steps的讨论，如果实际上accum_steps足够大（如256），那么数据通信的频率非常低。

还有等待成本，如果一些卡跑的更快，那么在模型更新的时候需要等待最慢的卡完成它的任务。但是在这么低的同步频率下，这个成本的占比也非常小。

所以，DP几乎能达到N倍的加速比上限！

> 如果开了grad offload，那么每次forward-backward都需要收集一次梯度，虽然通信成本增加了，但是仍然能一次处理8批数据，仍然具有相当高的加速比（7.xx倍）。
>
> 但是如果训练数据变长很严重的话，等待成本可能会增加。

然而，DP的前提是**每一张卡都有能力单独处理一批数据**。所以本教程前面对于单卡情况进行了大量讨论。

DP是几乎能达到N倍加速，那另外两种是不是更强呢？很遗憾，它们甚至连1倍都达不到，也就是比单卡更慢。

这也是为什么前面我们讨论了这么多时间与空间的权衡（事实上这是计算机科学的永恒话题）：
> 时间慢点就慢点，空间超过了就没法跑。
>
> 空间能用满必须用满，时间最好没有一点浪费。

> 小提示：torch要启动多卡不能使用python xx.py来启动，而要用torchrun --nproc-per-node=N xx.py来启动。或者其他框架会对这个包装一下，如deepspeed xx.py。

### PP：Pipeline Parallelism

**流水线并行**指的是，在一张卡没有能力供一个模型处理数据的时候，将模型按照层切分到多张卡上（深度模型就是多层模型），来共同处理一批数据。

很显然，首先它一次只能处理一批数据，其次在处理这批数据的时候，每当下一层跨卡就需要进行通信。因此，其时间加速比比1x还要低。

一个好消息是现代GPU通信速度非常快，但是再快也就是加速比（实际是减速）接近于1，和DP还是天壤之别。

因此，PP实际上是在**一张卡没有能力供一个模型**时的无奈之举，要是单卡能做到肯定选择DP。

在模型更新的时候，因为每张卡上有一个模型的部分，它们各自更新自己部分就ok，但是正如前面分析的，大accum_steps情况下，模型更新时的通信忽略不计也无伤大雅。

PP是否越多卡越好？显然不是，模型切的越碎，通信成本越高。省越多的卡出来，可以跑dp+pp。

> 时间慢点就慢点，空间超过了就没法跑。
>
> lsrl中的patch_for_multi_gpus.py给出了一个pp的实现。

除了通信成本，还要考虑等待成本。如果你了解真正的**流水线**概念，可能会说，流水线流起来以后基本没有等待成本，所有的部分都在干活。

但是，别忘了我们在训练的时候除了Forward还需要Backward，也就是说，一批数据全推完以前，前面完成工作的部分并不能丢掉之前的而加载下一批数据，而是只能等待，产生了大量等待成本。

> 很多工作在研究怎么降低pp的等待时间（降低空洞）。

但是Batch推理的时候例外，不需要Backward可以实现真正的流水线并行，这时才可以把等待成本降下来。

### TP：Tensor Parallelism

依照前面的分析，所有的并行策略都是为了在时空上做权衡。
我的看法是，如果不是为了训练100B以上的超级模型，那么这个词短期内可以完全不用管。

**张量并行**指的是，**一张卡连一个模型中的一个层的参数矩阵都放不下**的时候，将这个参数矩阵切成多块放到多张卡上。

那么要多大的模型，才能一个层的参数矩阵都超过80G呢？

和上面一样的分析，它甚至每一次模型中的运算都需要进行通信，有着最为严重的通信成本，也是在一个参数矩阵都如此巨大时的无奈之举。

至于什么72B或者更小的模型，你真的确定要把它切的每次运算都进行通信吗？

> TP相比PP有个好处，看上去流水线总有显卡在等待，而TP保证几乎任何时刻所有卡（包括这些卡之间的带宽）都满负荷运行。
>
> 由于卡间通讯确实比较快，在小心地权衡了卡间传输成本和带来的收益以后，TP确实也可以在稍小的模型上进行。
>
> 多机情况一般会混合使用，比如单机TP，多机DP。（要不也考虑下全DP？）

## RL

本教程主要以GRPO为例，讨论大模型RL的实现、多卡分配、性能瓶颈的问题，并非RL怎么提升模型推理能力。

现代大模型RL，实际上在模型参数更新部分和前述的SFT过程没有区别，只是其用于训练的数据，是使用正在训练的大模型自己生成的，大致上是如下阶段：

> Rollout阶段 -> 计算Reward阶段 -> 模型训练阶段。

- Rollout部分需要使用当前模型进行推理，产生大量的候选答案。同时在模型更新的时候也需要随着同步模型参数。
- Reward阶段需要对这些候选答案进行评分，我把Reference之类的计算也算在这个阶段。
- 模型训练阶段需要进行前述的Forward-Backward-Step。

这种分阶段流水线，其性能瓶颈的分析在于：
- 流水线是否能流起来
- 阶段中最慢的部分是性能瓶颈

首先，对于流水线是否能流起来的问题，唯一的坑在Rollout阶段的模型参数同步部分，需要读取最新的模型参数导致流水线没法流起来。

解决这个的方法有两种：
- 完全不管或者最多设置一点过期等待，反正RL的loss里面本来就有个特别脏眼睛的clip(o, 1+eps, 1-eps)就是为了解决生成模型不同步问题的（如果完全同步的话这个clip结果永远是1）。
- 把accum_steps设置得特别大，这样同步很久才会进行一次。

对于第二个问题，按照**流水线**的理论，这三个阶段的速度平衡的情况下是最好的。让我们来分析一下三个阶段的成本占用。

- Rollout阶段，需要使用模型进行大批量的生成，速度由vllm等框架的生成决定。但是大致上几十秒可以生成几十条数据。
- Reward阶段，这个会随着你的Reward需求有非常大的差异性。最简单的规则Reward几乎不需要时间，使用Reward model的情况需要用GPU进行推理（不过一般比生成要简洁很多），最麻烦的是环境模拟Reward，需要使用CPU和沙箱环境执行几十秒才完成**1条**。
- 训练阶段，按照lsrl的速度，单卡20秒就能对2w个tokens进行Forward-Backward，至于Step反正几百步才会来一次真的，不影响。

所以，分配实际上要按照具体Reward阶段的占用来考虑，以下例子都是14B模型全参训练：

- 例1：较短序列，规则Reward，带reference的loss，使用3张卡跑vllm，2张卡跑dp训练，1张3090跑reference，或者2张vllm，1张训练，1张3090跑reference。
- 例2：较短序列，Reward model，带reference，使用2张卡vllm，1张卡reward，1张3090跑reference，1张卡训练。
- 例3：Computer Use Online RL，中等序列，1张卡同时生成和训练，1张3090跑reference（其实从显存看全部同一张卡上也行，只是我3090多），瓶颈全在Online的执行上。

实际上，训练几乎总是比生成快，所以我一般都分配更多的卡给生成部分。而Reward部分如果需要模拟，会占用大量时间但不需要显卡，增加显卡没有任何帮助，需要先考虑增加虚拟环境沙箱。

> 在非瓶颈部分，各种offload都可以使用以降低资源占用。例如，reference的计算我使用cpu offload使得3090可以跑14B推理，见patch_for_cpu_offload.py。（有的算法直接把这个reference正则项去掉，那更省事。）
> 
> 各种算法就是loss计算改一点点的事情，重点还是整个生成-评估-训练循环。
> 
> vllm的batch生成很快，可以在vllm使用大batch。而Reward model理论上batch推理更快，但是不是瓶颈的情况下优化了也没用。
> 
> 框架默认accum_steps较小，真实跑效果请使用大accum_steps!

## 👏 Citation

If you find this useful, please consider staring this repo or citing our work as follows:

```
@misc{LSRL,
  author = {Jiaqing Liang},
  title = {LSRL: Memory Efficient Large Model Training Framework},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/lsdefine/lsrl}},
}
```
